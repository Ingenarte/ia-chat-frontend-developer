#docker compose -p ai-frontend-chat-service up -d --build
services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-frontend-chat-service
    env_file:
      - ./.env
    environment:
      - TZ=Europe/Rome
      - HOST=0.0.0.0
      - PORT=8000
      - OLLAMA_BASE_URL=http://ollama:11434
    ports:
      - '8000:8000'
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    build:
      context: ./ollama
    container_name: ai-ollama
    environment:
      - TZ=Europe/Rome
      - OLLAMA_MODEL=qwen2.5-coder:3b
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # Opcional para controlar uso de GPU por Ollama:
      # - OLLAMA_NUM_GPU=1
    ports:
      - '11434:11434'
    gpus: all
    restart: unless-stopped

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: ai-cloudflared
    command: tunnel --config /cloudflared/config.yml run
    depends_on:
      - backend
    volumes:
      - ./cloudflared/config.yml:/cloudflared/config.yml:ro
      - ./cloudflared/6197f875-4c6b-40a8-bc35-6c200467e1e2.json:/cloudflared/6197f875-4c6b-40a8-bc35-6c200467e1e2.json:ro
    environment:
      - TZ=Europe/Rome
    restart: unless-stopped
