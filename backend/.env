# Networking (Docker)
OLLAMA_BASE_URL=http://ollama:11434

# Model name (must match what llm.py reads)
OLLAMA_MODEL=qwen2.5-coder:3b

# Debug flag
DEBUG=1

# Timeouts
GENERATION_TIMEOUT_SECONDS=400
OLLAMA_TIMEOUT_SECONDS=3500

# Quality and retries (runner.py expects these prefixes GEN_*)
GEN_MIN_SCORE=0.80
GEN_MAX_RETRIES=5
GEN_RETRY_BASE_DELAY_SECONDS=1.0

DEFAULT_NUM_PREDICT=4096